{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- $P(A|B)=P(A)P(B|A) \\div P(B)$\n",
    "- it can be used for spam classifier\n",
    "- example: how whould we express the probability of an email being spam if it contains the word \"free\"?\n",
    "- P(Spam | Free) = P(Spam)P(Free | Spam) / P(Free)\n",
    "- The numerator is the probability of a message being spam and containing the word \"free\" (this is subtly different from what we're looking for)\n",
    "- The denominator is the overall probability of an email containing the word \"free\". (Equivalent to P(Free|Spam)P(Spam) + P(Free|Not Spam)P(Not Spam))\n",
    "- So together - this ratio is the % of emails with the word \"free\" that are spam.\n",
    "----------------\n",
    "What about other words?\n",
    "- We can construct P(Spam | Word) for every (meaningful) word we encounter during training\n",
    "- Then multiply these together when analyzing a new email to get the probability of it being spam.\n",
    "- Assumes the presence of different words are independent of each other - one reason this is called \"Naive Bayes\".\n",
    "----------------\n",
    "- Scikit-learn to the rescue!\n",
    "- The CountVectorizer lets us operate on lots of words at once, and MultinamiaNB does all the heavy lifting on Naive Bayes.\n",
    "- We'll train it on known sets of spam and \"ham\" (non-spam) emails\n",
    "    - So this is supervised learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
